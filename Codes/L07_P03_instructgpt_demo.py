# -*- coding: utf-8 -*-
"""InstructGPT demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zIdakeJNo041br2ViJ4FGYz4bbG24JIa
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import AutoModelForCausalLMWithValueHead, PPOTrainer, PPOConfig, RewardModel #transformers reinforcement learning(trl)

# Load a base GPT model
gpt_model = "gpt2"
model = AutoModelForCausalLMWithValueHead.from_pretrained(gpt_model)
tokenizer = AutoTokenizer.from_pretrained(gpt_model)

def reward_function(response: str) -> float:
    """Simulate a reward model by scoring response quality."""
    return len(response.split()) / 10  # Simple heuristic: longer responses are better

# PPO Configuration
config = PPOConfig(
    model_name=gpt_model,
    learning_rate=1.41e-5,
    batch_size=8,
    mini_batch_size=4,
    gradient_accumulation_steps=2,
    optimize_device=True
)

# Instantiate the PPO trainer
ppo_trainer = PPOTrainer(config, model, None, tokenizer)

# Example prompt
query = "Explain the importance of reinforcement learning"
query_tensor = tokenizer(query, return_tensors="pt").input_ids

# Generate a response
response_tensor = model.generate(query_tensor, max_length=50)
response = tokenizer.decode(response_tensor[0], skip_special_tokens=True)

# Compute reward
reward = reward_function(response)

# Train the model using PPO step
ppo_trainer.step([query_tensor], [response_tensor], torch.tensor([reward]))

print("Query:", query)
print("Response:", response)
print("Reward:", reward)

!pip install trl