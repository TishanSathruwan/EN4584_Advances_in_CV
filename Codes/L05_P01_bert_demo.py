# -*- coding: utf-8 -*-
"""BERT demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gn_wuohA_dfaUmaPBxXjtYX0zZWmA9Tr

## Hugging face implementation
"""

import torch
from transformers import BertTokenizer, BertModel, BertForMaskedLM

# Step 1: Tokenization
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

text = "the doctor treated the patient. [MASK] was very kind"
tokens = tokenizer.tokenize(text)  # Tokenize input text

input_ids = tokenizer.convert_tokens_to_ids(tokens)  # Convert tokens to IDs

# Add special tokens ([CLS] and [SEP])
input_ids = tokenizer.encode(text, return_tensors="pt")  # Add special tokens & convert to tensor

# Step 2: Generate Attention Mask (1 for real tokens, 0 for padding)
attention_mask = torch.ones(input_ids.shape)

# Step 3: Load Pretrained BERT Model
model = BertModel.from_pretrained("bert-base-uncased")

# Step 4: Forward Pass Through Transformer Encoder
with torch.no_grad():
    outputs = model(input_ids, attention_mask=attention_mask)

# Extract hidden states (last layer output)
last_hidden_states = outputs.last_hidden_state
print("Shape of last hidden state:", last_hidden_states.shape)  # (batch_size, seq_length, hidden_size)

# Step 5: Using BERT for Masked Language Model (MLM)
mlm_model = BertForMaskedLM.from_pretrained("bert-base-uncased")

with torch.no_grad():
    predictions = mlm_model(input_ids).logits  # Get prediction scores

# Get predicted word for [MASK]
mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]
predicted_token_id = torch.argmax(predictions[0, mask_token_index], dim=-1)
predicted_word = tokenizer.decode(predicted_token_id)

print(f"Predicted word for [MASK]: {predicted_word}")

print(tokens)

print(input_ids)

top_5_token_ids = torch.topk(predictions[0, mask_token_index], 5).indices[0]  # Top 5 token IDs
top_5_probs = torch.nn.functional.softmax(predictions[0, mask_token_index], dim=-1)[0][top_5_token_ids]  # Convert logits to probabilities

# Step 6: Decode Token IDs to Words & Print
top_5_words = tokenizer.convert_ids_to_tokens(top_5_token_ids)

print("Top 5 predictions for [MASK]:")
for word, prob in zip(top_5_words, top_5_probs):
    print(f"{word}: {prob:.4f}")